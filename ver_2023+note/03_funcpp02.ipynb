{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03_funcpp02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "功能像预处理的最核心部分。这一部分我们完成一下步骤\n",
    "* Despike\n",
    "* Slice-timing correction (时间层矫正)\n",
    "* Distortion correction (图像畸变矫正)\n",
    "* Motion correction (头动矫正)\n",
    "* align anat 2 epi (结构像到功能像的配准) \n",
    "* 把所有epi划到个体结构像空间\n",
    "* 把所有epi的volume数据划到surface data\n",
    "* 把所有epi的划到MNI标准空间\n",
    "* Scale (标准化数据尺度，抓换成percent signal change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Despike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================ despike =================================\n",
    "# 产生新的P01文件  带有despike标记\n",
    "# 这里应该利用了上一步生成的最小outlier volume信息， 同时基于pb00进行运算，生成pb01\n",
    "# 此时的pb01后缀为  r0*+despike+orig\n",
    "# 这一步仅生成pb01，不对前面的数据做修改\n",
    "# 然后删除pb00\n",
    "for run in runstr:\n",
    "    ! 3dDespike -NEW -nomask -prefix pb01.{subj}.r{run}.despike pb00.{subj}.r{run}.tcat+orig\n",
    "#! rm pb00*  #不建议删除pb00 因为总会出现奇怪的问题， 保留pb00可以从这里重新开始\n",
    "\n",
    "\n",
    "# pb00是未做despike的数据，仅仅做了平移，和T1像做了空间上的对齐， 而T1像已经和MNI做了对齐\n",
    "# pb00是一个立体的图像，包含了大脑，也包含大脑之外的组织"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Slice-timing correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================= tshift =================================\n",
    "# 因为每个tr里，扫描N层，  每一层扫描的时间不一样，并且可能扫描顺序是隔层或者逐层\n",
    "# 这里需要把每一层的扫描时间校正成同一个时间（可以指定以所有TR的某一层为准，或者按默认来）\n",
    "# 跑完后会变成pb02  后缀变为  tshift+orig  表示时间time发生了平移shift\n",
    "# 删除pb01\n",
    "for run in runstr:\n",
    "    ! 3dTshift -tzero 0 -quintic -prefix pb02.{subj}.r{run}.tshift \\\n",
    "         -verbose -tpattern @{'../SliceTiming.txt'} pb01.{subj}.r{run}.despike+orig\n",
    "\n",
    "# 删掉上一步despike的data节省空间\n",
    "! rm pb01*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Distortion correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为磁场不均匀的关系，图像会出现畸变。现在来做矫正。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里还没有使用场图\n",
    "# 使用正扫和反扫两两成对来互相校正\n",
    "# 这一步有3种方法， 1是使用场图， 2是AP-PA互校正 3是afni的方法纯粹从图像来进行校正（另一种AP-PA的用法？）\n",
    "# 这里生成blip_pre_*_mean, blip_post_*_mean， 可以用来进行校正效果的验证\n",
    "# 这里生成的blip_warp文件是主要所得，应该是对所有数据的校正结果\n",
    "# 如果关注前额叶， 则应该选用PA，这样的话前额叶部分会伸出去，然后再校正里压回来，这样信息不会丢失\n",
    "# 使用pb02， 生成blip_warp和blip_pre blip_post打头的文件\n",
    "\n",
    "for fwd,rev in zip(fwdfiles, revfiles):\n",
    "        # create median datasets from forward and reverse time series\n",
    "        ! 3dTstat -median -prefix rm.blip.med.fwd.r{fwd} pb02.{subj}.r{fwd}.tshift+orig\n",
    "        ! 3dTstat -median -prefix rm.blip.med.rev.r{rev} pb02.{subj}.r{rev}.tshift+orig\n",
    "        #对一个AP方向扫描的run的所有TR，取一个中位数\n",
    "        #对一个PA方向扫描的run的所有TR，取一个中位数\n",
    "\n",
    "        # automask the median datasets \n",
    "        ! 3dAutomask -apply_prefix rm.blip.med.masked.fwd.r{fwd} rm.blip.med.fwd.r{fwd}+orig\n",
    "        ! 3dAutomask -apply_prefix rm.blip.med.masked.rev.r{rev} rm.blip.med.rev.r{rev}+orig\n",
    "        # 使用3dAutomask命令， 对前面获取的中位数结果剥头皮，并且使用apply_prefix命令应用剥头皮结果，得到的是剥头皮后的结果（其值是有意义的激活值而不是0或1）\n",
    "\n",
    "        # compute the midpoint warp between the median datasets\n",
    "        ! 3dQwarp -plusminus -pmNAMES Rev.r{rev} Fwd.r{fwd}                           \\\n",
    "                -pblur 0.05 0.05 -blur -1 -1                          \\\n",
    "                -noweight -minpatch 9                                 \\\n",
    "                -noXdis -noZdis                                       \\\n",
    "                -source rm.blip.med.masked.rev.r{rev}+orig                   \\\n",
    "                -base   rm.blip.med.masked.fwd.r{fwd}+orig                   \\\n",
    "                -prefix blip_warp\n",
    "        \n",
    "        ! 3dNwarpApply -quintic -nwarp blip_warp_Fwd.r{fwd}_WARP+orig        \\\n",
    "                -source rm.blip.med.masked.fwd.r{fwd}+orig               \\\n",
    "                -prefix rm.blip.med.masked.fwd.post.r{fwd}\n",
    "\n",
    "        ! 3dNwarpApply -quintic -nwarp blip_warp_Rev.r{rev}_WARP+orig        \\\n",
    "                -source rm.blip.med.masked.rev.r{rev}+orig               \\\n",
    "                -prefix rm.blip.med.masked.rev.post.r{rev}\n",
    "        \n",
    "        # 删掉多余文件\n",
    "        ! rm blip_warp_Fwd.r{fwd}+orig* blip_warp_Rev.r{rev}+orig* \n",
    "\n",
    "        # 修改个名字以便以后的操作\n",
    "        ! 3dcopy blip_warp_Fwd.r{fwd}_WARP+orig blip_warp.r{fwd}_WARP+orig\n",
    "        ! 3dcopy blip_warp_Rev.r{rev}_WARP+orig blip_warp.r{rev}_WARP+orig\n",
    "\n",
    "        ! rm blip_warp_Fwd.r{fwd}+orig* blip_warp_Rev.r{rev}+orig* blip_warp_Fwd.r{fwd}_WARP+orig* blip_warp_Rev.r{rev}_WARP+orig* \n",
    "\n",
    "#此处生成的blip_warp.r* 是distortion correction的转换矩阵，每一个run都对应了一个矩阵  \n",
    "# 在后面将被用于把所有的run的EPI从pb02的状态转换到中间值（也就是矫正了AP扫描和PA扫描带来的形变问题）\n",
    "\n",
    "# for QC check，我们生成一个校正前和矫正后的平均EPI数据来作为distortion correction的效果证明\n",
    "! 3dMean -prefix blip_pre_fwd.epi_mean.nii.gz rm.blip.med.masked.fwd.r*\n",
    "! 3dMean -prefix blip_pre_rev.epi_mean.nii.gz rm.blip.med.masked.rev.r*\n",
    "! 3dMean -prefix blip_post_fwd.epi_mean.nii.gz rm.blip.med.masked.fwd.post.r*\n",
    "! 3dMean -prefix blip_post_rev.epi_mean.nii.gz rm.blip.med.masked.rev.post.r*\n",
    "\n",
    "#此处的blip_post_fwd是所有fwd方向的epi的平均值，并且已经被剥了头皮，（但是剥的比较粗糙）\n",
    "\n",
    "# remove redundent file\n",
    "! rm rm*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一步完成之后，可以手动打开\n",
    "* blip_pre_fwd.epi_mean.nii.gz\n",
    "* blip_pre_rev.epi_mean.nii.gz\n",
    "* blip_post_fwd.epi_mean.nii.gz\n",
    "* blip_post_rev.epi_mean.nii.gz\n",
    "\n",
    "这四个文件也记录了对侧矫正的效果，只要有矫正效果，没有什么大问题即可。\n",
    "\n",
    "然后把所有的run都做correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在这里将配准用的计算结果应用到图像数据里\n",
    "# 结合前面生成的矫正数据 与 pb02数据\n",
    "# 生成新的pb03.blip\n",
    "for run in runstr:\n",
    "    ! 3dNwarpApply -quintic -nwarp blip_warp.r{run}_WARP+orig      \\\n",
    "            -source pb02.{subj}.r{run}.tshift+orig         \\\n",
    "            -prefix pb03.{subj}.r{run}.blip\n",
    "\n",
    "#使用上一步生成的blip_warp转换矩阵把pb02的EPI数据转换为pb03的EPI数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Motion correction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先做motion correction。但是我们并不用其结果，主要是得到mc的线性转换矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成几个文件：\n",
    "# mat.r0*.vr.aff12.1D\n",
    "# dfile.r0*.1D\n",
    "# rm.epi.\n",
    "# 可能对pb03做了修改， 因为有overwrite一步\n",
    "\n",
    "# ================================= volreg =================================\n",
    "base = 'vr_base_min_outlier+orig' #在前面找到的  也就是那个最小头动的tr\n",
    "\n",
    "# align each dset to base volume, then we separate whether we want to further align epi to anat\n",
    "for run in runstr:\n",
    "    # register each volume to the base image\n",
    "    ! 3dvolreg -verbose -zpad 1 -base {base} \\\n",
    "             -1Dfile dfile.r{run}.1D \\\n",
    "             -cubic \\\n",
    "             -1Dmatrix_save mat.r{run}.vr.aff12.1D \\  \n",
    "             -prefix rm.epi.nomask.r{run} pb03.{subj}.r{run}.blip+orig\n",
    "    # 这一步也生成了 rm.epi.nomask.r{run}， 这个数据是对pb03进行了motion correction操作后得到的， 同时这一步操作使用的motion correction转换矩阵也被保存了\n",
    "    # 此处的 rm.epi.nomask.r{run}数据是先做了distortion correct后的数据(即pb03)，再经过此处的motion correction所得的结果\n",
    "    # 这个数据在后续处理中用于生成pb04\n",
    "\n",
    "    # 在这里也保存了mat.r{run}.vr.aff12.1D作为motion correction操作的转换矩阵，\n",
    "    # 后续会和前面的distortion correction转换矩阵一起进行EPI到标准空间的转换， 但不会和distortion correction转换矩阵合并（因为这个矩阵是线性变换，而distortion correction转换矩阵非线性\n",
    "    \n",
    "\n",
    "    # create an all-1 dataset to mask the extents of the warp\n",
    "    ! 3dcalc -overwrite -a pb03.{subj}.r{run}.blip+orig -expr 1 -prefix rm.epi.all1\n",
    "    # 在这里获取pb03的mask，其名称为 rm.epi.all1， 方法是直接获取pb03里并把油数据的地方标记为1，没有数值的地方标记为0\n",
    "    # 注意，此处获取的pb03的all1数据并不是剥头皮的后的mask，因为pb03本身也不是一个被剥头皮的数据\n",
    "    # 此处的pb03是做了distortion correct后的数据，而在这里取的all1数据，只是EPI数据被裁掉了一些边角后的结果，不可以和剥头皮后的mask混淆\n",
    "    # \n",
    "\n",
    "    # warp the all-1 dataset for extents masking\n",
    "    ! 3dAllineate -base {base} \\\n",
    "                -input rm.epi.all1+orig \\\n",
    "                -1Dmatrix_apply mat.r{run}.vr.aff12.1D \\\n",
    "                -final NN -quiet \\\n",
    "                -prefix rm.epi.1.r{run}\n",
    "    #此处把rm.epi.all1作为输入， 应用motion correctio转换矩阵，把all1矩阵也做一次转换，然后得到rm.epi.1.r{run}\n",
    "    #注意这里的rm.epi.1.r{run}应该包含每一个run的信息，并且每个run下都是4D数据，其中包含每一个TR的all1结果\n",
    "\n",
    "    # make an extents intersection mask of this run across time domain\n",
    "    # this makes a mask that all volumes in this run have valid numbers\n",
    "    ! 3dTstat -min -prefix rm.epi.min.r{run} rm.epi.1.r{run}+orig\n",
    "    # 对rm.epi.1.r{run}做计算， 获取每一个run里所有TR的all1矩阵的交集，  以此确保这个run里的所有TR数据都是有效数据，不包含任何all1以外的数据\n",
    "    # 此处得到的rm.epi.min.r{run} 数据应该包含8个run，且每个run下都是3D数据，为一个0 1构成的all1矩阵\n",
    "    # 仍然要强调一下，此处的all1矩阵不是剥头皮的mask，而是一个去掉EPI边角后的结果， 而去掉EPI边角的原因是EPI数据被做过一次distortion correction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算更多的头动数值，以后可以用来GLM中的regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- deal with motion parameter -------------------\n",
    "import numpy as np\n",
    "# 生成pb04.subj.r0*+volreg+orig\n",
    "# 生成epi_mean.nii.gz\n",
    "# motion打头的几个文件\n",
    "# 生成mc文件夹，里面放了矫正结果比较\n",
    "# we take dmeaned motion (6), demean motion derivative (6) and their squares (12)\n",
    "# 合并 motion params\n",
    "\n",
    "! cat dfile.r*.1D > dfile_rall.1D\n",
    "# 计算去均值的motion parameters (for use in regression)\n",
    "! 1d_tool.py -infile dfile_rall.1D -set_nruns {nRuns} -demean -write motion_demean.1D\n",
    "# 计算一阶导数 (just to have)\n",
    "! 1d_tool.py -infile dfile_rall.1D -set_nruns {nRuns} -derivative -demean -write motion_deriv.1D\n",
    "# calculate the square of demean and derivative of motion parameters (just to have)\n",
    "# for resting preproc, we usually have 24 motion regressor (6motion+6deriv+12 their square)\n",
    "np.savetxt('motion_demeansq.1D', np.loadtxt('motion_demean.1D')**2)\n",
    "np.savetxt('motion_derivsq.1D', np.loadtxt('motion_deriv.1D')**2)\n",
    "# create censor file motion_${subj}_censor.1D, for censoring motion\n",
    "! 1d_tool.py -infile dfile_rall.1D -set_nruns {nRuns} \\\n",
    "   -show_censor_count -censor_prev_TR \\\n",
    "   -censor_motion {motion_censor} motion_{subj}\n",
    "\n",
    "\n",
    "# Estimate the motion parameter after motion correction, we can check the\n",
    "# effects of MC\n",
    "! mkdir mc\n",
    "for run in runstr:\n",
    "    ! 3dvolreg -verbose -zpad 1 -base {base} \\\n",
    "             -1Dfile dfile.r{run}_pos.1D \\\n",
    "             rm.epi.nomask.r{run}+orig\n",
    "    ! rm volreg+orig*\n",
    "#此处用rm.epi.nomask.r{run}+orig数据生成一些检查信息，用来判断预处理的效果, 意义不大\n",
    "\n",
    "# make a single file of motion params\n",
    "! cat dfile.r*_pos.1D > dfile_rall_pos.1D\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# make figure pre mc\n",
    "dfile_pre = np.loadtxt('dfile_rall.1D')\n",
    "plot(range(dfile_pre.shape[0]), dfile_pre[:,:3], color=['C0','C1','C2'], label=['roll(IS)','pitch(RL)','yaw(AP)'])\n",
    "plt.legend();plt.xlabel('time points');plt.ylabel('mm');\n",
    "plt.savefig('rots_pre.pdf');plt.close('all')\n",
    "\n",
    "plot(range(dfile_pre.shape[0]), dfile_pre[:,3:], color=['C3','C4','C5'], label=['dS','dL','dP'])\n",
    "plt.legend();plt.xlabel('time points (TR)');plt.ylabel('mm');\n",
    "plt.savefig('tran_pre.pdf');plt.close('all')\n",
    "# make figure pos mc\n",
    "dfile_pos = np.loadtxt('dfile_rall_pos.1D')\n",
    "plot(range(dfile_pos.shape[0]), dfile_pos[:,:3], color=['C0','C1','C2'], label=['roll(IS)','pitch(RL)','yaw(AP)'])\n",
    "plt.legend();plt.xlabel('time points');plt.ylabel('mm');\n",
    "plt.savefig('rots_pos.pdf');plt.close('all')\n",
    "plot(range(dfile_pos.shape[0]), dfile_pos[:,3:], color=['C3','C4','C5'], label=['dS','dL','dP'])\n",
    "plt.legend();plt.xlabel('time points (TR)');plt.ylabel('mm');\n",
    "plt.savefig('tran_pos.pdf');plt.close('all')\n",
    "\n",
    "# move file to directory\n",
    "#将绘图结果和头动校正数据保存到mc里\n",
    "! mv rots*.pdf tran*.pdf dfile.r*_pos.1D dfile_r*_pos.1D mc/\n",
    "\n",
    "# note TRs that were not censored, note ktrs here is a str\n",
    "ktrs = unix_wrapper(f'1d_tool.py -infile motion_{subj}_censor.1D \\\n",
    "                       -show_trs_uncensored encoded', wantreturn=True, verbose=0)\n",
    "\n",
    "# ----------------------------------------\n",
    "# create the extents mask: mask_epi_extents+orig and apply the task\n",
    "# (this is a mask of voxels that have valid data at every TR,\n",
    "# there might be some pixel out of extents during mc)\n",
    "! 3dMean -datum short -prefix rm.epi.mean rm.epi.min.r*.HEAD\n",
    "# 对8个run的EPI的最小mask取一个均值，得到rm.epi.mean\n",
    "\n",
    "\n",
    "! 3dcalc -a rm.epi.mean+orig -expr \"step(a-0.999)\" -prefix mask_epi_extents\n",
    "# 猜测： 可能由于前一步生成rm.epi.mean数据时候是把8个run做了平均，因此存在部分区域的值是0-1之间的小数，而这些区域应当舍弃并变为0\n",
    "# 因此把上述rm.epi.mean数据全部减去0.999，然后把剩余大于0的数据全部变为1，而小于0的数据变为0\n",
    "# 这里生成的mask_epi_extents实际上是8个run的all1矩阵的交集，此处叫做mask_epi_extents有一定误导性，要注意这里的mask和剥头皮后的mask无关\n",
    "\n",
    "# and apply the extents mask to the EPI data\n",
    "# (delete any time series with missing data)\n",
    "for run in runstr:\n",
    "    ! 3dcalc -a rm.epi.nomask.r{run}+orig -b mask_epi_extents+orig \\\n",
    "           -expr \"a*b\" -prefix pb04.{subj}.r{run}.volreg\n",
    "! rm -f rm.*  # rm.epi.nomask are big files, remove them\n",
    "! rm -f {base}* \n",
    "# 这里对于前文生成的nomask数据————rm.epi.nomask.r{run}进行处理，得到pb04，  \n",
    "# 具体过程是把nomask数据和前面得到的mask_epi_extents相乘，\n",
    "# 目前得到的pb04已经过以下处理：  distortion correct、 motion correct、 对所有TR做裁剪， 使得8个run里的每一个TR里包含的脑子的尺寸和位置都可以重叠\n",
    "\n",
    "# calculate a mean epi volume for next step anat epi registration\n",
    "# 这是旧的方法，其过程是先对多个run进行run之间的平均，  然后再对平均run的结果进行时间维的平均， 但这么做存在问题，即要求所有run的长度必须一致，否则无法进行run间平均\n",
    "#! 3dMean -prefix rm.epi_mean.nii.gz pb04.{subj}.r*.volreg+orig.HEAD\n",
    "#! 3dTstat -prefix epi_mean.nii.gz rm.epi_mean.nii.gz\n",
    "\n",
    "# 以下是新方法， 先对所有run的结果分别做时间维的平均，然后再对平均后的多个结果进行run间平均\n",
    "for run in runstr:\n",
    "    ! 3dTstat -prefix rm.epi_mean_each_run.{subj}.r{run} pb04.{subj}.r{run}.volreg+orig.HEAD\n",
    "! 3dMean -prefix epi_mean.nii.gz rm.epi_mean_each_run.{subj}.r*\n",
    "# 把每一个run下的的所有TR的结果（pb04）进行时间维度的平均\n",
    "# 然后把所有run的平均结果再做平均， 由此得到一个3D数据，即所有EPI的平均值， 注意这里的平均结果仍然是没有做过剥头皮的\n",
    "\n",
    "\n",
    "! rm rm* mask_epi_extents+orig*\n",
    "\n",
    "# 删掉上一步distortion correction的数据，节省空间\n",
    "! rm pb03*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. anat2epi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一步非常重要，是前面处理好的功能像和结构像进行配准，配准的结果一定要手动检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先把用来配准的epi像make a copy\n",
    "! 3dcopy epi_mean.nii.gz epi_mean_tmp.nii.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 关键步骤。我们是把结构像配准到功能像，但是我们得到的转换矩阵是从相反的，功能像到结构像，这个矩阵可以后面用来转化epi数据\n",
    "# 生成了{subj}_SurfVol_al打头的文件\n",
    "# 生成了{subj}_SurfVol_ns打头的文件\n",
    "# 如果要重复进行这一步，需要删掉这两类文件\n",
    "\n",
    "\n",
    "! align_epi_anat.py -anat2epi -anat {t1.str} \\\n",
    "-save_skullstrip -suffix _al_junk \\\n",
    "-epi epi_mean_tmp.nii.gz -epi_base 0 \\\n",
    "-epi_strip 3dAutomask \\\n",
    "-volreg off -tshift off -giant_move\n",
    "# 此处输入的T1像（{t1.str}，以及平均epi像（epi_mean_tmp.nii.gz） 都是包含了头皮的数据\n",
    "# 此处的epi_strip命令，指定了3dAutomask方法对平均的EPI数据做去头皮，\n",
    "# 同时T1像也被默认做了剥头皮操作\n",
    "\n",
    "# 此处会得到{subj}_SurfVol_al_junk_mat.aff12.1D,  为结构像到功能像的转换矩阵， （ 注意代码中的命令“anat2epi”）\n",
    "# 得到 {subj}_SurfVol_al_junk.nii.gz   为T1像被配准到EPI的结果， \n",
    "# 得到 {subj}_SurfVol_ns.nii.gz   为T1像被剥头皮的结果\n",
    "\n",
    "\n",
    "# 生成nifti文件，方便我们在fsleyes上面查看配准效果\n",
    "! 3dcopy {subj}_SurfVol_al_junk+orig {subj}_SurfVol_al_junk.nii.gz\n",
    "# 此处把T1配准到EPI的结果转换为nii格式，然后在flseyes里打开，需要与epi_mean_tmp数据在进行对比检查两者是否一致，如果不匹配，则需要重新进行一次配准\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成上面一步之后，要打开afni仔细检查配准的结果，其中underlay选择CN003_SurfVol_al_junk.nii, overlay选择epi_mean_tmp+orig。然后检查两者是否一致。如果不匹配，则需要重新进行上面的过程，在开始之前，需要先删除上面产生的文件。可以做\n",
    "! rm {subj}_SurfVol_al* {subj}_SurfVol_ns*\n",
    "确认配准没有问题之后，进行下一步操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we align anat 2 epi, however, this xfm is from epi space to anat space\n",
    "# note that this xfm is compatible with LPS+ space, which is the default space in AFNI\n",
    "# 删除上一步生成的几个文件， 只保留 subj_SurfVol_al_junk.nii.gz\n",
    "# 生成anat_final   anat2epi  mat.epi2anat  epi2anat  volumized..等以及4张图片\n",
    "# 不清楚 mat.anat2epi.aff12.1D是什么时候生成的，看时间应该是上一步生成的，但看名称上一步里并没有它\n",
    "# 此处的t1和funcpp01的step1里的t1不同，  实际上的t1在step2里被更新了 后缀多了+orig\n",
    "\n",
    "! mv {t1.strnosuffix[:-5]}_al_junk_mat.aff12.1D mat.anat2epi.aff12.1D\n",
    "# 把前面生成的T1-EPI的转换矩阵重命名为mat.anat2epi.aff12.1D\n",
    "\n",
    "\n",
    "# create an skull-striped anat_final dataset, aligned with stats\n",
    "! 3dcopy {t1.strnosuffix[:-5]}_ns+orig anat_final.{subj}.nii.gz\n",
    "! rm {t1.strnosuffix[:-5]}_ns+orig*\n",
    "#把剥头皮后的T1像复制为anat_final.{subj}.nii.gz\n",
    "\n",
    "\n",
    "# rewrite the name of aligned anat\n",
    "! 3dcopy {t1.strnosuffix[:-5]}_al_junk+orig anat2epi.{subj}.nii.gz\n",
    "! rm {t1.strnosuffix[:-5]}_al_junk+orig*\n",
    "#把T1-EPI的配准结果重命名为  anat2epi.{subj}.nii.gz\n",
    "\n",
    "\n",
    "# Invert xfm\n",
    "! cat_matvec -ONELINE mat.anat2epi.aff12.1D -I > mat.epi2anat.aff12.1D\n",
    "# 把T1-EPI的转换矩阵反转， 得到EPI-T1的转换矩阵 mat.epi2anat.aff12.1D\n",
    "\n",
    "\n",
    "# 前面是将结构像绘制到EPI，现在是从EPI绘制到结构像\n",
    "# warp the volreg base EPI dataset back to anat to make a final version\n",
    "! 3dAllineate -base anat_final.{subj}.nii.gz \\\n",
    "            -input epi_mean.nii.gz \\\n",
    "            -1Dmatrix_apply mat.epi2anat.aff12.1D \\\n",
    "            -prefix epi2anat.{subj}.nii.gz\n",
    "# 基于被剥头皮后的T1像(似乎不需要这个base？ 不理解为什么，也许是用来做参考)\n",
    "# 输入的是平均epi结果：epi_mean.nii.gz\n",
    "# 基于epi2anat转换矩阵\n",
    "# 得到的是epi2anat.{subj}.nii.gz， 也就是平均epi被配准到T1像的结果\n",
    "# epi2anat.{subj}.nii.gz在下面被用于生成检查图片，方便查看转换结果是否正常\n",
    "\n",
    "\n",
    "# Record final registration costs\n",
    "! 3dAllineate -base epi2anat.{subj}.nii.gz -allcostX -input anat_final.{subj}.nii.gz > out.allcostX.txt\n",
    "# 此处意义不明，可能用于后期对数据预处理的质量做诊断\n",
    "\n",
    "# Take the snapshots to show the quality of alignment\n",
    "! @snapshot_volreg epi2anat.{subj}.nii.gz anat_final.{subj}.nii.gz\n",
    "! @snapshot_volreg anat_final.{subj}.nii.gz epi2anat.{subj}.nii.gz\n",
    "# 此处把 平均epi被配准到T1像的结果 epi2anat    与 T1的剥头皮结果anat_final  互相做了对比（互为上下）\n",
    "\n",
    "! @snapshot_volreg anat2epi.{subj}.nii.gz epi_mean.nii.gz\n",
    "! @snapshot_volreg epi_mean.nii.gz anat2epi.{subj}.nii.gz\n",
    "# 此处把T1配准到EPI的结果， 与 EPI均值 互相做了对比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. epi to individual anat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们有了epi到anat的配准，然后我们就可以把所有的epi的文件画到t1的空间。注意，这一步并不是从motion correction之后的文件，也就是pb03.CN003.r0X.volreg+orig。我们要从早distrotion correction之前的文件，把从DC-MC-epi2anat这三个转换一次性的做完"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成几个准备删除的文件，以及mat.r0*.warp.aff12.1D文件\n",
    "# \n",
    "resolution=2.4  #这个值指定了EPI被配准到T1后的体素尺寸，单位mm\n",
    "\n",
    "# ======== Transform epi to match anat, add by RZ ===============\n",
    "# In this step we need to concatenate Distortion(opt)+motion+affine transformations\n",
    "# align each dset to base volume, then we separate whether we want to further align epi to anat\n",
    "for run in runstr:\n",
    "    # concatenate MC+aff transforms\n",
    "    ! cat_matvec -ONELINE mat.anat2epi.aff12.1D -I mat.r{run}.vr.aff12.1D > mat.r{run}.warp.aff12.1D\n",
    "    # 首先，合并motion correction的转换矩阵：mat.r{run}.vr.aff12.1D， (注意这里每个run都有一个单独的MC转换矩阵, 再注意这里的命令 -I表示对MC转转矩阵做反转)\n",
    "    # 以及 T1到EPI的转换矩阵：mat.anat2epi.aff12.1D \n",
    "    # 因为这两个转换矩阵是线性的，因此可以直接合并， 最终生成 mat.r{run}.warp.aff12.1D,  每个run单独得到一个转换矩阵\n",
    "    # 这里的结果是  MC和anat2epi两个转换矩阵的总和（线性变换）\n",
    "    #尚不清楚为什么这里会指定 -I\n",
    "\n",
    "    # we apply distortion correction + motion correction + anat2epi transformation\n",
    "    cmd = f'3dNwarpApply -quintic -nwarp \"mat.r{run}.warp.aff12.1D blip_warp.r{run}_WARP+orig\" \\\n",
    "            -master {t1} -dxyz {resolution} \\\n",
    "            -source pb02.{subj}.r{run}.tshift+orig         \\\n",
    "            -prefix rm.epi.nomask.r{run}'\n",
    "    unix_wrapper(cmd)\n",
    "    \n",
    "    # 此处的命令：3dNwarpApply可执行非线性转换\n",
    "    # 这里把上一步的线性变换矩阵，以及之前生成的非线性变换矩阵blip_warp (也就是distortion correction矩阵)，一并输入unix_wrapper(cmd)进行变换\n",
    "    # 这里被变换的文件是pb02，也就是仅仅做了despike处理和Slice-timing correction处理的文件\n",
    "    # 最后会得到一个run内的EPI被配准到T1的结果\n",
    "    # 这里的mast指定的是该名被试的t1数据，yyq说这里要根据t1数据获取grid信息，最后生成的数据会和t1保持相同的grid\n",
    "\n",
    "\n",
    "    # create an all-1 dataset to mask the extents of the warp\n",
    "    ! 3dcalc -overwrite -a pb02.{subj}.r{run}.tshift+orig -expr 1 -prefix rm.epi.all1\n",
    "    # 然后对pb02获取一个all1矩阵，  道理同STEP4 motion correction\n",
    "\n",
    "    # we apply distortion correction + motion correction + anat2epi transformation\n",
    "    cmd = f'3dNwarpApply -quintic -nwarp \"mat.r{run}.warp.aff12.1D blip_warp.r{run}_WARP+orig\" \\\n",
    "            -master {t1} -dxyz {resolution} \\\n",
    "            -ainterp NN -quiet \\\n",
    "            -source rm.epi.all1+orig \\\n",
    "            -prefix rm.epi.1.r{run}'\n",
    "    unix_wrapper(cmd)\n",
    "    # 对all1矩阵也同样的做一次配准,\n",
    "\n",
    "    # make an extents intersection mask of this run across time domain\n",
    "    # this makes a mask that all volumes in this run have valid numbers\n",
    "    ! 3dTstat -min -prefix rm.epi.min.r{run} rm.epi.1.r{run}+orig\n",
    "        # 对转换后的all1矩阵取每个run下的交集 道理同STEP4 motion correction\n",
    "\n",
    "    # below file is big, should be removed\n",
    "    ! rm rm.epi.1.r{run}+orig*\n",
    "\n",
    "\n",
    "# 这部分代码的操作，实际上是把每一个被试的EPI数据，配准到了这个被试自己的T1像上\n",
    "# 如果要做组水平分析，这一步似乎用处不大\n",
    "# 组水平分析需要把每一个被试的EPI都配准到标准MNI空间里\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "做了变化之后，也要处理一下mask的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 生成pb05  mask_epi_extents+orig\n",
    "# create the extents mask: mask_epi_extents+orig\n",
    "# (this is a mask of voxels that have valid data at every TR,\n",
    "# there might be some pixel out of extents during mc)\n",
    "! 3dMean -datum short -prefix rm.epi.mean rm.epi.min.r*.HEAD\n",
    "# and apply the extents mask to the EPI data\n",
    "! 3dcalc -a rm.epi.mean+orig -expr \"step(a-0.999)\" -prefix mask_epi_extents\n",
    "#d 道理同STEP4 motion correction\n",
    "\n",
    "# 去掉mask之外的voxel的数据\n",
    "for run in runstr:\n",
    "    ! 3dcalc -a rm.epi.nomask.r{run}+orig -b mask_epi_extents+orig \\\n",
    "           -expr \"a*b\" -prefix pb05.{subj}.r{run}.al2anat\n",
    "    # save to nifti format seems to reduce file size\n",
    "# 道理同STEP4 motion correction，  注意这里的mask不是剥头皮后的mask\n",
    "# 确保每一个RUN下的所有TR的脑子的位置和大小都是一致的\n",
    "\n",
    "# rm.epi.nomask are big files, remove them\n",
    "! rm rm.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. epi volume 2 surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上一步我们已经把epi数据划到了和个体的结构像一个空间。很多时候我们需要做基于surface的数据分析。我们进一步把三维的epi数据插值划到surface上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成 pb05.subj.rh.r0*.surf.niml.dset\n",
    "# 生成 pb05.subj.lh.r0*.surf.niml.dset\n",
    "\n",
    "FREESURFER_HOME = os.getenv('FREESURFER_HOME') # 获得 FreeSurfer的主文件夹路径\n",
    "surface_dir = FREESURFER_HOME+f'/subjects/{subj}/SUMA'\n",
    "# map volume data to the surface of each hemisphere\n",
    "for hemi in ['lh', 'rh']:\n",
    "    for  run in runstr:\n",
    "        ! 3dVol2Surf -spec {surface_dir}/std.141.{subj}_{hemi}.spec   \\\n",
    "                   -sv {subj}_SurfVol+orig           \\\n",
    "                   -surf_A smoothwm                            \\\n",
    "                   -surf_B pial                                \\\n",
    "                   -f_index nodes                              \\\n",
    "                   -f_steps 10                                 \\\n",
    "                   -map_func ave                               \\\n",
    "                   -oob_value 0                                \\\n",
    "                   -grid_parent pb05.{subj}.r{run}.al2anat+orig   \\\n",
    "                   -out_niml pb05.{subj}.{hemi}.r{run}.surf.niml.dset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8. epi to MNI anat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Transform epi to match anat, add by RZ ===============\n",
    "# In this step we need to concatenate Distortion(opt)+motion+affine transformations\n",
    "# align each dset to base volume, then we separate whether we want to further align epi to anat\n",
    "# 生成许多待删除的文件，  rm.epi...\n",
    "\n",
    "for run in runstr:\n",
    "\n",
    "    # we apply distortion correction + motion correction + anat2epi + nonlinear warp to MNI transformation\n",
    "    # 注意我们这里是相反的顺序来concatenate的\n",
    "    cmd = f'3dNwarpApply -quintic -nwarp \"anatQQ.{subj}_WARP.nii anatQQ.{subj}.aff12.1D mat.r{run}.warp.aff12.1D blip_warp.r{run}_WARP+orig\" \\\n",
    "            -master MNI152_2009_template_SSW.nii.gz \\\n",
    "            -dxyz {resolution} \\\n",
    "            -source pb02.{subj}.r{run}.tshift+orig \\\n",
    "            -prefix rm.epi.nomask.r{run}'\n",
    "    unix_wrapper(cmd)\n",
    "        # 此处需要输入：\n",
    "        # anatQQ.{subj}_WARP.nii  T1-MNI 的非线性转换矩阵（需确认）\n",
    "        # anatQQ.{subj}.aff12.1D  T1-MNI 的线性转换矩阵（需确认）\n",
    "        # mat.r{run}.warp.aff12.1D   T1-EPI的线性转换矩阵 结合  EPI的motion correction线性转换矩阵\n",
    "        # blip_warp.r{run}_WARP+orig  EPI做distortion correction的非线性转换矩阵\n",
    "        # 疑问，这里的几个转换矩阵的方向很怪，  好像不能直接用来做转换， 是否要对其中的几个矩阵也做一次反转处理？ 应该需要检查一下unix_wrapper函数的内容\n",
    "\n",
    "        # 这里被转换的数据是pb02，也就是仅仅做了despike处理和Slice-timing correction处理的文件\n",
    "        # master指定的是MNI标准空间模板（不清楚其作用是什么，pb02直接经过转换后就可以得到最终结果了吧？为什么还要指定MNI？）yyq说这里要根据MNI数据获取grid信息，最后生成的数据会和MNI保持相同的grid\n",
    "\n",
    "        #这里得到的nomask数据是EPI数据被转换到MNI标准空间里的数据， 注意这里的nomask指的是没有被裁剪过，而不是没有被剥头皮（确实没有剥头皮）\n",
    "\n",
    "\n",
    "    # create an all-1 dataset to mask the extents of the warp\n",
    "    ! 3dcalc -overwrite -a pb02.{subj}.r{run}.tshift+orig -expr 1 -prefix rm.epi.all1\n",
    "        #此处获取all1矩阵，道理同上\n",
    "\n",
    "    # we apply distortion correction + motion correction + anat2epi transformation\n",
    "    cmd = f'3dNwarpApply -quintic -nwarp \"anatQQ.{subj}_WARP.nii anatQQ.{subj}.aff12.1D mat.r{run}.warp.aff12.1D blip_warp.r{run}_WARP+orig\" \\\n",
    "            -master MNI152_2009_template_SSW.nii.gz \\\n",
    "            -dxyz {resolution} \\\n",
    "            -source rm.epi.all1+orig         \\\n",
    "            -ainterp NN -quiet \\\n",
    "            -prefix rm.epi.1.r{run}'\n",
    "    unix_wrapper(cmd)\n",
    "    #此处对all1矩阵做同样的变换，道理同上\n",
    "\n",
    "    # make an extents intersection mask of this run across time domain\n",
    "    # this makes a mask that all volumes in this run have valid numbers\n",
    "    ! 3dTstat -min -prefix rm.epi.min.r{run} rm.epi.1.r{run}+tlrc\n",
    "        #此处对all1矩阵，取每个run下的所有TR里的交集\n",
    "\n",
    "    # below file is big, should be removed\n",
    "    ! rm rm.epi.1.r{run}+tlrc*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "做了变化之后，也要处理一下mask的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 生成pb06.subj.r0*.al2mni+tlrc\n",
    "# 以及mask_epi_2mni_extents+tlrc\n",
    "# create the extents mask: mask_epi_extents+orig\n",
    "# (this is a mask of voxels that have valid data at every TR,\n",
    "# there might be some pixel out of extents during mc)\n",
    "! 3dMean -datum short -prefix rm.epi.mean rm.epi.min.r*.HEAD\n",
    "# and apply the extents mask to the EPI data\n",
    "! 3dcalc -a rm.epi.mean+tlrc -expr \"step(a-0.999)\" -prefix mask_epi_2mni_extents\n",
    "#使用熟悉的操作，获取8个run下的所有TR的交集\n",
    "\n",
    "# 去掉mask之外的voxel的数据， 注意这里的mask仍然和剥头皮无关\n",
    "for run in runstr:\n",
    "    ! 3dcalc -a rm.epi.nomask.r{run}+tlrc -b mask_epi_2mni_extents+tlrc \\\n",
    "           -expr \"a*b\" -prefix pb06.{subj}.r{run}.al2mni\n",
    "    # save to nifti format seems to reduce file size\n",
    "\n",
    "# rm.epi.nomask are big files, remove them\n",
    "! rm rm.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 补充步骤，用于进行smooth操作，  2022.7.3添加\n",
    "# 目的是对上一步step8所生成的pb06进行smooth   生成pb06s\n",
    "# 再对上上一步step7所生成的pb05***surf.niml文件进行smooth  生成pb05s*rh  和 pb05s*lh\n",
    "# 再对上上一步step7所生成的pb05.subj.r0*.al2anat+orig文件进行smooth  生成pb05s*.subj.r0*.blur+al2anat+orig\n",
    "# pb05是配准到个体的T1的EPI\n",
    "# pb06是配准到标准MNI的EPI\n",
    "\n",
    "for run in runstr:\n",
    "    ! 3dmerge -1blur_fwhm 4.0 -doall -prefix pb06s.{subj}.r{run}.blur+al2mni+tlrc \\\n",
    "        pb06.{subj}.r{run}.al2mni+tlrc\n",
    "\n",
    "for run in runstr:\n",
    "    ! 3dmerge -1blur_fwhm 4.0 -doall -prefix pb05s.{subj}.lh.r{run}.blur.surf.niml.dset \\\n",
    "        pb05.{subj}.lh.r{run}.surf.niml.dset\n",
    "\n",
    "for run in runstr:\n",
    "    ! 3dmerge -1blur_fwhm 4.0 -doall -prefix pb05s.{subj}.rh.r{run}.blur.surf.niml.dset \\\n",
    "        pb05.{subj}.rh.r{run}.surf.niml.dset\n",
    "\n",
    "#以下代码可以不用跑\n",
    "for run in runstr:\n",
    "    ! 3dmerge -1blur_fwhm 4.0 -doall -prefix pb05s.{subj}.r{run}.blur+al2anat+orig \\\n",
    "        pb05.{subj}.r{run}.al2anat+orig\n",
    "\n",
    "\n",
    "'''  \n",
    "#afni官网上的对于surface数据进行smooth的代码\n",
    "\n",
    "foreach hemi ( lh rh )\n",
    "    foreach run ( $runs )\n",
    "        # to save time, estimate blur parameters only once\n",
    "        if ( ! -f surf.smooth.params.1D ) then\n",
    "            SurfSmooth -spec $surface_dir/std.60.FT_${hemi}.spec         \\\n",
    "                       -surf_A smoothwm                                  \\\n",
    "                       -input pb03.$subj.$hemi.r$run.surf.niml.dset      \\\n",
    "                       -met HEAT_07                                      \\\n",
    "                       -target_fwhm 6.0                                  \\\n",
    "                       -blurmaster pb03.$subj.$hemi.r$run.surf.niml.dset \\\n",
    "                       -detrend_master                                   \\\n",
    "                       -output pb04.$subj.$hemi.r$run.blur.niml.dset     \\\n",
    "                       | tee surf.smooth.params.1D\n",
    "        else\n",
    "            set params = `1dcat surf.smooth.params.1D`\n",
    "            SurfSmooth -spec $surface_dir/std.60.FT_${hemi}.spec         \\\n",
    "                       -surf_A smoothwm                                  \\\n",
    "                       -input pb03.$subj.$hemi.r$run.surf.niml.dset      \\\n",
    "                       -met HEAT_07                                      \\\n",
    "                       -Niter $params[1]                                 \\\n",
    "                       -sigma $params[2]                                 \\\n",
    "                       -output pb04.$subj.$hemi.r$run.blur.niml.dset\n",
    "        endif\n",
    "    end\n",
    "end\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9. 创造一个所有epi和mni模板共有的mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Mask =================================\n",
    "# 这里创造mask所用的信息是pb06， 而不是前一步的pb06s\n",
    "\n",
    "# Create 'full_mask' dataset (union mask)\n",
    "for run in runstr:\n",
    "    ! 3dAutomask -prefix rm.mask_r{run}.nii.gz pb06.{subj}.r{run}.al2mni+tlrc\n",
    "# 这里对pb06的数据，逐个run的剥头皮\n",
    "# 这里应该是对一个run剥一次头皮就好了吧， 也可能会对每个TR单独剥一次头皮？\n",
    "# 这里生成的rm.mask_r{run}.nii.gz 是剥头皮后的mask数据（值为0或1），注意和apply_prefix命令不同\n",
    "\n",
    "\n",
    "# 对所有run下的mask文件取并集，得到full_mask\n",
    "! 3dmask_tool -inputs rm.mask_r*.nii.gz -union -prefix full_mask.{subj}.nii.gz\n",
    "! rm rm.mask*\n",
    "\n",
    "# ---- create subject anatomy mask, mask_anat.$subj+orig ----\n",
    "#      (resampled from aligned anat)\n",
    "\n",
    "# 把MNI图像resample到full_mask上\n",
    "! 3dresample -master full_mask.{subj}.nii.gz -input \\\n",
    "           MNI152_2009_template_SSW.nii.gz -prefix rm.resam.anat.nii.gz\n",
    "\n",
    "\n",
    "# convert to binary anat mask; fill gaps and holes\n",
    "! 3dmask_tool -dilate_input 5 -5 -fill_holes -input rm.resam.anat.nii.gz \\\n",
    "            -prefix mask_anat.{subj}.nii.gz\n",
    "#对resample之后的MNI数据取一次mask，得到mask_anat.{subj}.nii.gz， 即结构像的mask\n",
    "# fill_holes命令表示把可能存在的空隙填充为1\n",
    "# dilate_input命令表示把生成的mask做一个扩充，使得其能够包裹更多区间（甚至包裹到一部分大脑之外的组织），提高冗余\n",
    "# ！！！！！！！！！！这里需要确认，因为得到的mask_anat.{subj}.nii.gz数据，看起来像是对整个头颅的mask，而不是对大脑的mask\n",
    "# 按理说前一步得到的rm.resam.anat.nii.gz结果应该已经是接近fullmask的样子才对\n",
    "\n",
    "\n",
    "\n",
    "# 结合功能像和结构像的mask\n",
    "# compute tighter EPI mask by intersecting with anat mask\n",
    "! 3dmask_tool -input full_mask.{subj}.nii.gz mask_anat.{subj}.nii.gz \\\n",
    "            -inter -prefix mask_epi_anat.{subj}.nii.gz\n",
    "# 对基于EPI获取的full_mask以及 对MNI做了resample后的mask数据，取一个交集，得到最终mask数据：mask_epi_anat.{subj}.nii.gz \n",
    "\n",
    "# note Dice coefficient of masks, as well\n",
    "! 3ddot -dodice full_mask.{subj}.nii.gz mask_anat.{subj}.nii.gz > out.mask_ae_dice.txt\n",
    "#意义不明\n",
    "\n",
    "# 删掉多余文件\n",
    "! rm rm*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10. scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始EPI图像的数据可能很大，从几百到4000不等。但是我们一般不考虑这个绝对强度，考虑的是信号增长的比例。所以把数据scale一下, 让所有的voxel的时间序列均值为100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意！！！这个cell的代码是对pb06以及pb05 进行scale重置 也就是未经smooth的数据\n",
    "# scale each voxel time series to have a mean of 100\n",
    "# (be sure no negatives creep in)\n",
    "# (subject to a range of [0,200])\n",
    "# scale volume data\n",
    "\n",
    "for run in runstr:\n",
    "    ! 3dTstat -prefix rm.mean_r{run} pb06.{subj}.r{run}.al2mni+tlrc\n",
    "    ! 3dcalc -a pb06.{subj}.r{run}.al2mni+tlrc -b rm.mean_r{run}+tlrc \\\n",
    "           -c mask_epi_2mni_extents+tlrc \\\n",
    "           -expr \"c * min(200, a/b*100)*step(a)*step(b)\" \\\n",
    "           -prefix pb07.{subj}.r{run}.scale\n",
    "\n",
    "# combine all datasets into one\n",
    "! 3dTcat -prefix all_runs.{subj}.nii.gz pb07.{subj}.r*.scale+tlrc*\n",
    "# remove redundant file\n",
    "! rm rm.mean*\n",
    "\n",
    "# 再scale surface data\n",
    "for hemi in ['lh', 'rh']:\n",
    "    for run in runstr:\n",
    "       ! 3dTstat -prefix rm.{hemi}.mean_r{run}.niml.dset    \\\n",
    "            pb05.{subj}.{hemi}.r{run}.surf.niml.dset\n",
    "       ! 3dcalc -a pb05.{subj}.{hemi}.r{run}.surf.niml.dset  \\\n",
    "               -b rm.{hemi}.mean_r{run}.niml.dset          \\\n",
    "               -expr 'min(200, a/b*100)*step(a)*step(b)' \\\n",
    "               -prefix pb07.{subj}.{hemi}.r{run}.scale.niml.dset\n",
    "\n",
    "# remove redundant file\n",
    "! rm rm.*mean* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意！！！这个cell的代码是对pb06s 以及pb05s 进行scale重置 也就是经过了smooth的数据\n",
    "# 生成pb07s*.scale  \n",
    "# 这个文件被整合成all_runs_with_smooth\n",
    "# 生成pb07s*.scale.niml.dset\n",
    "\n",
    "# scale volume data\n",
    "for run in runstr:\n",
    "    ! 3dTstat -prefix rm.mean_r{run} pb06s.{subj}.r{run}.blur+al2mni+tlrc\n",
    "    ! 3dcalc -a pb06s.{subj}.r{run}.blur+al2mni+tlrc -b rm.mean_r{run}+tlrc \\\n",
    "           -c mask_epi_2mni_extents+tlrc \\\n",
    "           -expr \"c * min(200, a/b*100)*step(a)*step(b)\" \\\n",
    "           -prefix pb07s.{subj}.r{run}.scale\n",
    "\n",
    "# combine all datasets into one\n",
    "! 3dTcat -prefix all_runs_with_smooth.{subj}.nii.gz pb07s.{subj}.r*.scale+tlrc*\n",
    "# remove redundant file\n",
    "! rm rm.mean*\n",
    "\n",
    "# 再scale surface data\n",
    "for hemi in ['lh', 'rh']:\n",
    "    for run in runstr:\n",
    "       ! 3dTstat -prefix rm.{hemi}.mean_r{run}.niml.dset    \\\n",
    "            pb05s.{subj}.{hemi}.r{run}.blur.surf.niml.dset\n",
    "       ! 3dcalc -a pb05s.{subj}.{hemi}.r{run}.blur.surf.niml.dset  \\\n",
    "               -b rm.{hemi}.mean_r{run}.niml.dset          \\\n",
    "               -expr 'min(200, a/b*100)*step(a)*step(b)' \\\n",
    "               -prefix pb07s.{subj}.{hemi}.r{run}.scale.niml.dset\n",
    "# remove redundant file\n",
    "! rm rm.*mean* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到这一步，基本上最主要的预处理就完结了。下一步可以进一步针对surface数据跑GLM，因为afni的surface都是在freesurfer的模板上对其且标准化的，所以surface数据可以直接跑group analysis。也可以在已经划到MNI space上的volume数据跑GLM。以后在volume上跑GLM的结果可以直接用MNI152的T1像来可视化"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e737c620b82cbe435429c6717e50da63bcd664f8b8a5a02fd0f297629db2d8c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
