{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organized data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 上传数据\n",
    "* 连接拷回来的光盘，然后利用ftp软件连接porter之后，上传数据。\n",
    "* 所有的原始数据，都放到 /home/data/rawdata 文件夹下，每一个session用一个文件夹表示，通常为 ‘年月日_实验缩写_被试代号’的格式，例如“20220110_CET_CN003”\n",
    "* 在该文件夹下，新建一个 `mat_files_from_exp`的文件夹用来存放所有的scan里面的行为数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  打开terminal, 开放权限让别人也能读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/data/rawdata/CET_fmri/20221102_CET_CN016/\n",
    "! chgrp -R student 20221102_CET_CN016 #修改该文件夹的所属组别\n",
    "! chmod g+w 20221102_CET_CN016 #修改该文件夹的权限"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 整理数据  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! dcm2niix_afni -f %t_%s_%p rawdata/\n",
    "# afni函数dcm2niix_afni -[filename] time_seriesnumber_protocol\n",
    "# output: .nii文件是nifit格式MRI数据文件，.json文件是对应的参数说明文件\n",
    "\n",
    "# 整理无关数据：原始的dicom文件，localizer，以及费的结构或者没扫完的功能像等\n",
    "! cd rawfiles/\n",
    "! mkdir rawfiles\n",
    "! mv *localizer* rawfiles\n",
    "! mv *Localizer* rawfiles\n",
    "! mv *Dicom* rawfiles\n",
    "# 一般的fMRI实验只需要留下t1、epi和field-map的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anatprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置一些variable\n",
    "import os\n",
    "t1 = '20221209111810_6_t1_mpr_sag_iso_mww.nii'  # set the t1 file\n",
    "subj = 'CN022' # 设定一个variable\n",
    "\n",
    "t1_2FS = f'{subj}_2FS.nii.gz' \n",
    "AFNI_HOME = os.getenv('AFNIDIR')\n",
    "FREESURFER_HOME = os.getenv('FREESURFER_HOME') # 获得 FreeSurfer的主文件夹路径\n",
    "MNIT1 = AFNI_HOME + '/MNI152_2009_template_SSW.nii.gz' # 我们用MNI152去掉头皮的模板作为我们的base\n",
    "\n",
    "\n",
    "%cd ../ # 我们先走到数据文件夹\n",
    "! mkdir anatpp # 新建一个文件夹用来处理t1像\n",
    "! cp rawdata/{t1} anatpp/ # 把t1像拷贝一份到文件夹里面\n",
    "%cd anatpp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. step1: 对t1像进行简单处理  \n",
    "     \n",
    "   得到 {t1_2FS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 改变nifti头文件信息，进行标准化，不修改数据\n",
    "# 把图像的中点划到统一显示空间的中点\n",
    "! 3drefit -deoblique -xorigin cen -yorigin cen -zorigin cen {t1}\n",
    "# 把图像的朝向划到和标准朝向一致,这一步生成t12std.nii.gz的文件\n",
    "! fslreorient2std {t1} t1_2std.nii.gz \n",
    "\n",
    "\n",
    "# 简单的剥头皮\n",
    "! 3dSkullStrip -input t1_2std.nii.gz -prefix t1_2std_ss.nii.gz -orig_vol\n",
    "\n",
    "# 上面一步把剥掉头皮的t1像和MNI模板按重心位置移动，同时移动t12std.nii.gz, 这样做是为了让其和MNI模板空间上更一致。\n",
    "! @Align_Centers -cm -base {MNIT1} -dset t1_2std_ss.nii.gz -child t1_2std.nii.gz\n",
    "\n",
    "\n",
    "# 清除一下\n",
    "! mv t1_2std_shft.nii.gz {t1_2FS} \n",
    "! rm *t1_2std* # 删除掉多余文件\n",
    "\n",
    "# 先检查/home/data/subjectT1/{subj} 是否存在，如果没有，创建一个文件夹\n",
    "! mkdir /home/data/subjectT1/{subj}\n",
    "# 把FreeSurfer的T1文件拷贝进去\n",
    "! cp {t1_2FS} /home/data/subjectT1/{subj}/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一步需要利用fsleyes来人工确认（在跑FreeSurfer之前）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. step2: 把{t1_2FS}送给freesurfer进行皮层重建  \n",
    "     \n",
    "    recon-all 会在/home/software/freesurfer/subjects下面创建一个{subj}名字的文件夹，例如CN003。如果该文件夹已经存在(例如recon-all中途中断，但是重新跑一次)都会报错。这时删除之前的旧文件夹就行了。所有的报错信息都在reconlog.txt这个文件下。\n",
    "\n",
    "    这一步可能会要很长时间。在stout上面测试跑单个被试需要6个小时。。这一步所有的输出都在reconlog.txt里面\n",
    "    这一步跑完之后。把非线性配准的结果文件移动到FreeSurfer被试文件夹下面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/data/subjectT1/{subj}\n",
    "! recon-all -s {subj} -i {subj}_2FS.nii.gz -all > ./reconlog.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成之后检查：  \n",
    "/home/software/freesurfer/7.3.2/subjects/{subj}/mri  \n",
    "* ***brainmask.mgz***  \n",
    "* ***T1.mgz***\n",
    "\n",
    "freeview:  \n",
    "***brainmask.nii.gz***  \n",
    "***lh_pial***, ***rh_pial***  \n",
    "***lh_white***,  ***rh_white***  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. step3: 将t1结构像线性和非线性配准到mni空间  \n",
    "     \n",
    "    将个体的t1像配准到mni标准空间和mni152的标准模板，这是基于volume的分析的基础\n",
    "   可以和step2平行跑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重开一个terminal，重新设置变量\n",
    "\n",
    "cd /home/data/subjectT1/{subj}\n",
    "\n",
    "import os\n",
    "subj = 'CN022' # 设定一个variable\n",
    "t1_2FS = f'{subj}_2FS.nii.gz' \n",
    "AFNI_HOME = os.getenv('AFNIDIR')\n",
    "FREESURFER_HOME = os.getenv('FREESURFER_HOME') # 获得 FreeSurfer的主文件夹路径\n",
    "MNIT1 = AFNI_HOME + '/MNI152_2009_template_SSW.nii.gz' # 我们用MNI152去掉头皮的模板作为我们的base\n",
    "\n",
    "\n",
    "#cd /home/data/subjectT1/{subj}\n",
    "! mkdir mni # make a new directory\n",
    "! @SSwarper -input {t1_2FS} -base {MNIT1} -subid {subj} -odir mni -verb\n",
    "\n",
    "# 创建一个剥头皮的mask，如果剥头皮效果不满意，可以手动编辑这个mask，然后再手动剥头皮\n",
    "%cd mni/\n",
    "! 3dcalc -a anatSS.{subj}.nii -expr 'step(a)' -prefix anatSSmask.{subj}.nii\n",
    "print('Nonlinear warping done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用afni做基于volume的非线性配准，完成之后需要人工检查配准结果  \n",
    "* ***anatSS.{sub}.nii***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. step4: afni中皮层加工模块SUMA的设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把step3中间的非线性配准的文件夹放到freesurfer里面去\n",
    "! mv /home/data/subjectT1/{subj}/mni {FREESURFER_HOME}/subjects/{subj}/ # 移动step3获得到mni文件夹到step2生成的文件夹\n",
    "\n",
    "% cd /home/software/freesurfer/7.3.2/subjects/{subj}/\n",
    "# AFNI中用来进行皮层加工模块的是SUMA。利用FreeSurfer进行了皮层重建之后，需要对SUMA进行一定的设置\n",
    "! @SUMA_Make_Spec_FS -sid {subj} -fspath {FREESURFER_HOME}/subjects/{subj} -NIFTI > SUMA_Make_Spec_FS.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上的命令会在FreeSurfer subjects中再创建一个SUMA的文件夹，里面包含了SUMA需要的一些文件。SUMA是连接AFNI和FreeSurfer之间的工具，让在AFNI上进行皮层分析成为可能。\n",
    "\n",
    "这一步可能需要10分钟左右。完成之后，我们打开远程桌面，打开一个terminal (注意这一步是bash命令，不用ipython)，输入以下的命令，可以打开afni和suma来检查互相的联通情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意替换被试名\n",
    "cd $FREESURFER_HOME/subjects/CN003/SUMA\n",
    "afni -niml & suma -spec CN003_both.spec -sv CN003_SurfVol.nii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funcprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RZutilpy.system import Path, unix_wrapper, gettimestr, makedirs\n",
    "from RZutilpy.rzio import matchfiles\n",
    "from RZutilpy.figure import plot\n",
    "from RZutilpy.mri import findminoutlier\n",
    "import os\n",
    "\n",
    "# subject name\n",
    "subj = 'CN026'\n",
    "\n",
    "# a list of functional NIFTI files\n",
    "files = matchfiles(f'/home/data/rawdata/CET_fmri/20221208_CET_CN026/rawdata/*epi*.nii')\n",
    "\n",
    "# t1 file\n",
    "t1 = Path(f'/home/software/freesurfer/7.3.2/subjects/{subj}/SUMA/{subj}_SurfVol.nii')\n",
    "t1ss = Path(f'/home/software/freesurfer/7.3.2/subjects/{subj}/mni/anatSS.{subj}.nii')\n",
    "\n",
    "'''\n",
    "###################如果是中途回来重新设置\n",
    "t1 = Path(f'{t1.pstem}+orig')  # switch t1 to the new location\n",
    "FREESURFER_HOME = os.getenv('FREESURFER_HOME')\n",
    "AFNI_HOME = os.getenv('AFNIDIR')\n",
    "cwd = Path.cwd() # record current directory, we will go back\n",
    "orig_shell = os.environ['SHELL']\n",
    "'''\n",
    "\n",
    "# output directory, if exists, we exit\n",
    "output_dir = Path(f'/home/data/rawdata/CET_fmri/20221208_CET_CN026/funcpp/')\n",
    "\n",
    "# number of tr to discard\n",
    "tr_discard = 0  # number of tr to discard\n",
    "\n",
    "# motion censor limit\n",
    "motion_censor = 0.3  # threshold for motion censoring, default:(0.3)\n",
    "\n",
    "# extra option for align_epi_anat.py\n",
    "align_opt = ['-giant_move']\n",
    "\n",
    "fwdfiles = [ '01', '03','05','07'] # 这里需要手动run的顺序，用来做配对distortion correction\n",
    "revfiles = ['02','04','06','08']\n",
    "\n",
    "# some calculation\n",
    "nRuns = len(files)\n",
    "runstr = [f'{i+1:02d}' for i in range(nRuns)]############################################################\n",
    "motion_censor = 0.3 if motion_censor is None else motion_censor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funcpp01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 检查文件，设置文件，复制文件  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查文件\n",
    "# ============== deal with some input parameters =======================\n",
    "# Show some diagnostic information\n",
    "# check file exists\n",
    "\n",
    "#cd rawdata/\n",
    "\n",
    "for i in files:\n",
    "    assert Path(i).exists(), f'{i} does not exist!'\n",
    "\n",
    "t1 = Path(t1)\n",
    "assert t1.exists(), 'T1 file does not exist!'\n",
    "\n",
    "\n",
    "\n",
    "# generate slice timing file\n",
    "if not Path('SliceTiming.txt').exists():\n",
    "    import json \n",
    "    import numpy as np\n",
    "    with open(f'{Path(files[0]).pstem}.json') as f:\n",
    "        jsoninfo = json.load(f)\n",
    "    np.savetxt('SliceTiming.txt', jsoninfo['SliceTiming'])\n",
    "    print('generate SliceTiming.txt!')\n",
    "    del jsoninfo\n",
    "\n",
    "# 整理forward和reverse的文件，如果两者数量不相等，我们自动补齐缺数量\n",
    "if len(fwdfiles)!=len(fwdfiles):\n",
    "    if len(fwdfiles)>len(revfiles):\n",
    "        nPair = len(fwdfiles)\n",
    "        revfiles = revfiles + [revfiles[end]]*(nPair-len(revfiles))\n",
    "    else:\n",
    "        nPair = len(revfiles)\n",
    "        fwdfiles = fwdfiles + [fwdfiles[end]]*(nPair-len(fwdfiles))\n",
    "\n",
    "# print out some diagnoistic\n",
    "print(f'\\nt1 file is: \\n{t1}')\n",
    "print(f'functional files are: \\n')\n",
    "[print(f'{i}') for i in files]\n",
    "print(f'\\nforward files are: {fwdfiles} \\n')\n",
    "print(f'reverse files are: {revfiles} \\n')\n",
    "print(f'\\nslice timing file is: \\nSliceTiming.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进一步设置\n",
    "start_time = gettimestr(\"full\")\n",
    "print(f'\\n=============== Preprocessing started: {start_time} ================\\n')\n",
    "# change shell to tcsh, afni default shell is tcsh\n",
    "orig_shell = os.environ['SHELL']\n",
    "os.environ['SHELL']='/usr/bin/tcsh'\n",
    "cwd = Path.cwd() # record current directory, we will go back\n",
    "\n",
    "# verify that the results directory does not yet exist\n",
    "output_dir = Path(output_dir)\n",
    "assert not output_dir.exists(), f'output dir {output_dir} already exists'\n",
    "makedirs(output_dir)\n",
    "#makedirs((output_dir / 'stimuli'))\n",
    "# enter the results directory (can begin processing data)\n",
    "os.chdir(output_dir)\n",
    "# copy anatomy to results dir\n",
    "! 3dcopy {t1} {t1.pstem}\n",
    "t1 = Path(f'{t1.pstem}+orig')  # switch t1 to the new location\n",
    "\n",
    "# copy nonlinear-warping MNI files to this folder\n",
    "FREESURFER_HOME = os.getenv('FREESURFER_HOME')\n",
    "AFNI_HOME = os.getenv('AFNIDIR')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "! cp {FREESURFER_HOME}/subjects/{subj}/mni/anatQQ.{subj}_WARP.nii ./\n",
    "! cp {FREESURFER_HOME}/subjects/{subj}/mni/anatQQ.{subj}.aff12.1D ./\n",
    "! cp {AFNI_HOME}/MNI152_2009_template_SSW.nii.gz ./\n",
    "\n",
    "# 利用fslreorient2std把朝向划到和MNI一样\n",
    "for file in files:\n",
    "    ! fslreorient2std {file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 把所有的功能像数据先copy到一个<output_dir>文件夹下, 避免修改原始的功能像数据, 并且去掉每个run前面的几个TR <tr_discard>  \n",
    "  **----pb00.{subj}.r{run}.tcat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 复制文件到<output_dir>，同时去掉最开始几个TR\n",
    "# ============================ auto block: tcat ============================\n",
    "# apply 3dTcat to copy input dsets to results dir,\n",
    "# while might want removing the first TRs\n",
    "for file, run in zip(files, runstr):\n",
    "    cmd=f'3dTcat -prefix pb00.{subj}.r{run}.tcat {file}[{tr_discard}..$]'\n",
    "    unix_wrapper(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * 矫正epi的朝向和正中点  \n",
    "   其中如果epi和T1是在不同的session采集的，那么可能会有比较大的偏差。需要手动的移动epi的图像到和用来做FreeSurfer的t1像一致  \n",
    "  **----pb00.{subj}.r{run}.tcat+orig**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移动EPI文件，和T1中心重合\n",
    "# ============================auto block: giant_move, added by RZ ===========================\n",
    "for run in runstr:\n",
    "    # 把图像的中点划到统一显示空间的中点\n",
    "    ! 3drefit -deoblique -xorigin cen -yorigin cen -zorigin cen pb00.{subj}.r{run}.tcat+orig\n",
    "    # 然后把function数据和 t1ss文件的重心移动到一致，这样有利于进行配准\n",
    "    ! @Align_Centers -cm -no_cp -base {t1ss} -dset pb00.{subj}.r{run}.tcat+orig\n",
    "\n",
    "! rm *_shft.1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * 到<output_dir> 文件夹下面，找到头动最小的volume，然后把这个作为头动矫正的基准。这一步会检查头动，并且记录一些特别大头动的volume，以头动超过一定的threshold为准"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同时找到outlier最小的一个volume作为后面motion correction的base\n",
    "findminoutlier(f'pb00.{subj}.r*.tcat+orig.HEAD', output_dir)\n",
    "# this part will generate several files\n",
    "#   out.pre_ss_warn.txt: warning for pre-steady state in the first TRs, consider change tr_discard\n",
    "#   outcont.r**.1D: fraction of outlier in each volume\n",
    "#   outcount_rall.1D concatenate fraction of outlier\n",
    "#   out.min_outlier.txt  tells you which run, which TR is min_outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funcpp02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. step1: Despike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================ despike =================================\n",
    "for run in runstr:\n",
    "    ! 3dDespike -NEW -nomask -prefix pb01.{subj}.r{run}.despike pb00.{subj}.r{run}.tcat+orig\n",
    "    \n",
    "#! rm pb00*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. step 2: slice-timing correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================= tshift =================================\n",
    "for run in runstr:\n",
    "    ! 3dTshift -tzero 0 -quintic -prefix pb02.{subj}.r{run}.tshift \\\n",
    "         -verbose -tpattern @{'../rawdata/SliceTiming.txt'} pb01.{subj}.r{run}.despike+orig\n",
    "\n",
    "# 删掉上一步despike的data节省空间\n",
    "! rm pb01*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. step3: distortion correction  \n",
    "  \n",
    "   因为磁场不均匀的关系，图像会出现畸变。现在来做矫正。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先找到\n",
    "for fwd,rev in zip(fwdfiles, revfiles):\n",
    "        # create median datasets from forward and reverse time series\n",
    "        ! 3dTstat -median -prefix rm.blip.med.fwd.r{fwd} pb02.{subj}.r{fwd}.tshift+orig\n",
    "        ! 3dTstat -median -prefix rm.blip.med.rev.r{rev} pb02.{subj}.r{rev}.tshift+orig\n",
    "        # automask the median datasets \n",
    "        ! 3dAutomask -apply_prefix rm.blip.med.masked.fwd.r{fwd} rm.blip.med.fwd.r{fwd}+orig\n",
    "        ! 3dAutomask -apply_prefix rm.blip.med.masked.rev.r{rev} rm.blip.med.rev.r{rev}+orig\n",
    "        # compute the midpoint warp between the median datasets\n",
    "        ! 3dQwarp -plusminus -pmNAMES Rev.r{rev} Fwd.r{fwd}                           \\\n",
    "                -pblur 0.05 0.05 -blur -1 -1                          \\\n",
    "                -noweight -minpatch 9                                 \\\n",
    "                -noXdis -noZdis                                       \\\n",
    "                -source rm.blip.med.masked.rev.r{rev}+orig                   \\\n",
    "                -base   rm.blip.med.masked.fwd.r{fwd}+orig                   \\\n",
    "                -prefix blip_warp\n",
    "        \n",
    "        ! 3dNwarpApply -quintic -nwarp blip_warp_Fwd.r{fwd}_WARP+orig        \\\n",
    "                -source rm.blip.med.masked.fwd.r{fwd}+orig               \\\n",
    "                -prefix rm.blip.med.masked.fwd.post.r{fwd}\n",
    "\n",
    "        ! 3dNwarpApply -quintic -nwarp blip_warp_Rev.r{rev}_WARP+orig        \\\n",
    "                -source rm.blip.med.masked.rev.r{rev}+orig               \\\n",
    "                -prefix rm.blip.med.masked.rev.post.r{rev}\n",
    "        \n",
    "        # 删掉多余文件\n",
    "        ! rm blip_warp_Fwd.r{fwd}+orig* blip_warp_Rev.r{rev}+orig* \n",
    "\n",
    "        # 修改个名字以便以后的操作\n",
    "        ! 3dcopy blip_warp_Fwd.r{fwd}_WARP+orig blip_warp.r{fwd}_WARP+orig\n",
    "        ! 3dcopy blip_warp_Rev.r{rev}_WARP+orig blip_warp.r{rev}_WARP+orig\n",
    "\n",
    "        ! rm blip_warp_Fwd.r{fwd}+orig* blip_warp_Rev.r{rev}+orig* blip_warp_Fwd.r{fwd}_WARP+orig* blip_warp_Rev.r{rev}_WARP+orig* \n",
    "\n",
    "# for QC check，我们生成一个校正前和矫正后的平均EPI数据来作为distortion correction的效果证明\n",
    "! 3dMean -prefix blip_pre_fwd.epi_mean.nii.gz rm.blip.med.masked.fwd.r*\n",
    "! 3dMean -prefix blip_pre_rev.epi_mean.nii.gz rm.blip.med.masked.rev.r*\n",
    "! 3dMean -prefix blip_post_fwd.epi_mean.nii.gz rm.blip.med.masked.fwd.post.r*\n",
    "! 3dMean -prefix blip_post_rev.epi_mean.nii.gz rm.blip.med.masked.rev.post.r*\n",
    "\n",
    "# remove redundent file\n",
    "! rm rm*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一步完成之后，可以手动打开\n",
    "* blip_pre_fwd.epi_mean.nii.gz\n",
    "* blip_pre_rev.epi_mean.nii.gz\n",
    "* blip_post_fwd.epi_mean.nii.gz\n",
    "* blip_post_rev.epi_mean.nii.gz\n",
    "\n",
    "这四个文件也记录了对侧矫正的效果，只要有矫正效果，没有什么大问题即可。\n",
    "\n",
    "然后把所有的run都做correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in runstr:\n",
    "    ! 3dNwarpApply -quintic -nwarp blip_warp.r{run}_WARP+orig      \\\n",
    "            -source pb02.{subj}.r{run}.tshift+orig         \\\n",
    "            -prefix pb03.{subj}.r{run}.blip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. step 4: motion correction  \n",
    "  \n",
    "    首先做motion correction。但是我们并不用其结果，主要是得到mc的线性转换矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================= volreg =================================\n",
    "base = 'vr_base_min_outlier+orig' #在前面生成\n",
    "\n",
    "# align each dset to base volume, then we separate whether we want to further align epi to anat\n",
    "for run in runstr:\n",
    "    # register each volume to the base image\n",
    "    ! 3dvolreg -verbose -zpad 1 -base {base} \\\n",
    "             -1Dfile dfile.r{run}.1D \\\n",
    "             -cubic \\\n",
    "             -1Dmatrix_save mat.r{run}.vr.aff12.1D \\\n",
    "             -prefix rm.epi.nomask.r{run} pb03.{subj}.r{run}.blip+orig\n",
    "\n",
    "    # create an all-1 dataset to mask the extents of the warp\n",
    "    ! 3dcalc -overwrite -a pb03.{subj}.r{run}.blip+orig -expr 1 -prefix rm.epi.all1\n",
    "\n",
    "    # warp the all-1 dataset for extents masking\n",
    "    ! 3dAllineate -base {base} \\\n",
    "                -input rm.epi.all1+orig \\\n",
    "                -1Dmatrix_apply mat.r{run}.vr.aff12.1D \\\n",
    "                -final NN -quiet \\\n",
    "                -prefix rm.epi.1.r{run}\n",
    "\n",
    "    # make an extents intersection mask of this run across time domain\n",
    "    # this makes a mask that all volumes in this run have valid numbers\n",
    "    ! 3dTstat -min -prefix rm.epi.min.r{run} rm.epi.1.r{run}+orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算更多的头动数值，以后可以用来GLM中的regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- deal with motion parameter -------------------\n",
    "# we take dmeaned motion (6), demean motion derivative (6) and their squares (12)\n",
    "# 合并 motion params\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "! cat dfile.r*.1D > dfile_rall.1D\n",
    "# 计算去均值的motion parameters (for use in regression)\n",
    "! 1d_tool.py -infile dfile_rall.1D -set_nruns {nRuns} -demean -write motion_demean.1D\n",
    "# 计算一阶导数 (just to have)\n",
    "! 1d_tool.py -infile dfile_rall.1D -set_nruns {nRuns} -derivative -demean -write motion_deriv.1D\n",
    "# calculate the square of demean and derivative of motion parameters (just to have)\n",
    "# for resting preproc, we usually have 24 motion regressor (6motion+6deriv+12 their square)\n",
    "np.savetxt('motion_demeansq.1D', np.loadtxt('motion_demean.1D')**2)\n",
    "np.savetxt('motion_derivsq.1D', np.loadtxt('motion_deriv.1D')**2)\n",
    "# create censor file motion_${subj}_censor.1D, for censoring motion\n",
    "! 1d_tool.py -infile dfile_rall.1D -set_nruns {nRuns} \\\n",
    "   -show_censor_count -censor_prev_TR \\\n",
    "   -censor_motion {motion_censor} motion_{subj}\n",
    "\n",
    "# Estimate the motion parameter after motion correction, we can check the\n",
    "# effects of MC\n",
    "! mkdir mc\n",
    "for run in runstr:\n",
    "    ! 3dvolreg -verbose -zpad 1 -base {base} \\\n",
    "             -1Dfile dfile.r{run}_pos.1D \\\n",
    "             rm.epi.nomask.r{run}+orig\n",
    "    ! rm volreg+orig*\n",
    "\n",
    "# make a single file of motion params\n",
    "\n",
    "! cat dfile.r*_pos.1D > dfile_rall_pos.1D\n",
    "\n",
    "# make figure pre mc\n",
    "dfile_pre = np.loadtxt('dfile_rall.1D')\n",
    "plot(range(dfile_pre.shape[0]), dfile_pre[:,:3], color=['C0','C1','C2'], label=['roll(IS)','pitch(RL)','yaw(AP)'])\n",
    "plt.legend();plt.xlabel('time points');plt.ylabel('mm');\n",
    "plt.savefig('rots_pre.pdf');plt.close('all')\n",
    "plot(range(dfile_pre.shape[0]), dfile_pre[:,3:], color=['C3','C4','C5'], label=['dS','dL','dP'])\n",
    "plt.legend();plt.xlabel('time points (TR)');plt.ylabel('mm');\n",
    "plt.savefig('tran_pre.pdf');plt.close('all')\n",
    "# make figure pos mc\n",
    "dfile_pos = np.loadtxt('dfile_rall_pos.1D')\n",
    "plot(range(dfile_pos.shape[0]), dfile_pos[:,:3], color=['C0','C1','C2'], label=['roll(IS)','pitch(RL)','yaw(AP)'])\n",
    "plt.legend();plt.xlabel('time points');plt.ylabel('mm');\n",
    "plt.savefig('rots_pos.pdf');plt.close('all')\n",
    "plot(range(dfile_pos.shape[0]), dfile_pos[:,3:], color=['C3','C4','C5'], label=['dS','dL','dP'])\n",
    "plt.legend();plt.xlabel('time points (TR)');plt.ylabel('mm');\n",
    "plt.savefig('tran_pos.pdf');plt.close('all')\n",
    "\n",
    "# move file to directory\n",
    "\n",
    "! mv rots*.pdf tran*.pdf dfile.r*_pos.1D dfile_r*_pos.1D mc/\n",
    "\n",
    "# note TRs that were not censored, note ktrs here is a str\n",
    "ktrs = unix_wrapper(f'1d_tool.py -infile motion_{subj}_censor.1D -show_trs_uncensored encoded', wantreturn=True, verbose=0)\n",
    "\n",
    "# ----------------------------------------\n",
    "# create the extents mask: mask_epi_extents+orig and apply the task\n",
    "# (this is a mask of voxels that have valid data at every TR,\n",
    "# there might be some pixel out of extents during mc)\n",
    "! 3dMean -datum short -prefix rm.epi.mean rm.epi.min.r*.HEAD\n",
    "! 3dcalc -a rm.epi.mean+orig -expr \"step(a-0.999)\" -prefix mask_epi_extents\n",
    "# and apply the extents mask to the EPI data\n",
    "# (delete any time series with missing data)\n",
    "for run in runstr:\n",
    "    ! 3dcalc -a rm.epi.nomask.r{run}+orig -b mask_epi_extents+orig \\\n",
    "           -expr \"a*b\" -prefix pb04.{subj}.r{run}.volreg\n",
    "! rm -f rm.*  # rm.epi.nomask are big files, remove them\n",
    "! rm -f {base}* \n",
    "\n",
    "# calculate a mean epi volume for next step anat epi registration\n",
    "! 3dMean -prefix rm.epi_mean.nii.gz pb04.{subj}.r*.volreg+orig.HEAD\n",
    "! 3dTstat -prefix epi_mean.nii.gz rm.epi_mean.nii.gz\n",
    "! rm rm* mask_epi_extents+orig*\n",
    "\n",
    "# 删掉上一步distortion correction的数据，节省空间\n",
    "! rm pb03*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. step 5: anat2epi  \n",
    "  \n",
    "    这一步非常重要，是前面处理好的功能像和结构像进行配准，配准结果一定要手动检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先把用来配准的epi像make a copy\n",
    "! 3dcopy epi_mean.nii.gz epi_mean_tmp.nii.gz\n",
    "\n",
    "# 关键步骤。我们是把结构像配准到功能像，但是我们得到的转换矩阵是从相反的，功能像到结构像，这个矩阵可以后面用来转化epi数据\n",
    "# \n",
    "! align_epi_anat.py -anat2epi -anat {t1.str} \\\n",
    "-save_skullstrip -suffix _al_junk \\\n",
    "-epi epi_mean_tmp.nii.gz -epi_base 0 \\\n",
    "-epi_strip 3dAutomask \\\n",
    "-volreg off -tshift off\n",
    "\n",
    "# 生成nifti文件，方便我们在fsleyes上面查看配准效果\n",
    "! 3dcopy {subj}_SurfVol_al_junk+orig {subj}_SurfVol_al_junk.nii.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成上面一步之后，要打开afni仔细检查配准的结果，其中underlay选择 ***CN003_SurfVol_al_junk+orig***,  overlay选择 ***epi_mean_tmp+orig***，然后检查两者是否一致。  \n",
    "如果不匹配，则需要重新进行上面的过程，在开始之前，需要先删除上面产生的文件。可以做  \n",
    "    ! rm {subj}_SurfVol_al* {subj}_SurfVol_ns*\n",
    "确认配准没有问题之后，进行下一步操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we align anat 2 epi, however, this xfm is from epi space to anat space\n",
    "# note that this xfm is compatible with LPS+ space, which is the default space in AFNI\n",
    "! mv {t1.strnosuffix[:-5]}_al_junk_mat.aff12.1D mat.anat2epi.aff12.1D\n",
    "# create an skull-striped anat_final dataset, aligned with stats\n",
    "! 3dcopy {t1.strnosuffix[:-5]}_ns+orig anat_final.{subj}.nii.gz\n",
    "! rm {t1.strnosuffix[:-5]}_ns+orig*\n",
    "# rewrite the name of aligned anat\n",
    "! 3dcopy {t1.strnosuffix[:-5]}_al_junk+orig anat2epi.{subj}.nii.gz\n",
    "! rm {t1.strnosuffix[:-5]}_al_junk+orig*\n",
    "\n",
    "# Invert xfm\n",
    "! cat_matvec -ONELINE mat.anat2epi.aff12.1D -I > mat.epi2anat.aff12.1D\n",
    "\n",
    "# warp the volreg base EPI dataset back to anat to make a final version\n",
    "! 3dAllineate -base anat_final.{subj}.nii.gz \\\n",
    "            -input epi_mean.nii.gz \\\n",
    "            -1Dmatrix_apply mat.epi2anat.aff12.1D \\\n",
    "            -prefix epi2anat.{subj}.nii.gz\n",
    "\n",
    "# Record final registration costs\n",
    "! 3dAllineate -base epi2anat.{subj}.nii.gz -allcostX -input anat_final.{subj}.nii.gz > out.allcostX.txt\n",
    "\n",
    "# Take the snapshots to show the quality of alignment\n",
    "! @snapshot_volreg epi2anat.{subj}.nii.gz anat_final.{subj}.nii.gz\n",
    "! @snapshot_volreg anat_final.{subj}.nii.gz epi2anat.{subj}.nii.gz\n",
    "! @snapshot_volreg anat2epi.{subj}.nii.gz epi_mean.nii.gz\n",
    "! @snapshot_volreg epi_mean.nii.gz anat2epi.{subj}.nii.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. step 6: epi to individual anat\n",
    "  \n",
    "    现在我们有了epi到anat的配准，然后我们就可以把所有的epi的文件划到t1的空间。注意，这一步并不是从motion correction之后的文件(pb03.CN003.r0X.volreg+orig)。我们要从早distrotion correction之前的文件，把从DC-MC-epi2anat这三个转换一次性的做完"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution=2.5\n",
    "# ======== Transform epi to match anat, add by RZ ===============\n",
    "# In this step we need to concatenate Distortion(opt)+motion+affine transformations\n",
    "# align each dset to base volume, then we separate whether we want to further align epi to anat\n",
    "for run in runstr:\n",
    "    # concatenate MC+aff transforms\n",
    "    ! cat_matvec -ONELINE mat.anat2epi.aff12.1D -I mat.r{run}.vr.aff12.1D > mat.r{run}.warp.aff12.1D\n",
    "\n",
    "    # we apply distortion correction + motion correction + anat2epi transformation\n",
    "    cmd = f'3dNwarpApply -quintic -nwarp \"mat.r{run}.warp.aff12.1D blip_warp.r{run}_WARP+orig\" \\\n",
    "            -master {t1} -dxyz {resolution} \\\n",
    "            -source pb02.{subj}.r{run}.tshift+orig         \\\n",
    "            -prefix rm.epi.nomask.r{run}'\n",
    "    unix_wrapper(cmd)\n",
    "\n",
    "    # create an all-1 dataset to mask the extents of the warp\n",
    "    ! 3dcalc -overwrite -a pb02.{subj}.r{run}.tshift+orig -expr 1 -prefix rm.epi.all1\n",
    "\n",
    "    # we apply distortion correction + motion correction + anat2epi transformation\n",
    "    cmd = f'3dNwarpApply -quintic -nwarp \"mat.r{run}.warp.aff12.1D blip_warp.r{run}_WARP+orig\" \\\n",
    "            -master {t1} -dxyz {resolution} \\\n",
    "            -ainterp NN -quiet \\\n",
    "            -source rm.epi.all1+orig \\\n",
    "            -prefix rm.epi.1.r{run}'\n",
    "    unix_wrapper(cmd)\n",
    "\n",
    "    # make an extents intersection mask of this run across time domain\n",
    "    # this makes a mask that all volumes in this run have valid numbers\n",
    "    ! 3dTstat -min -prefix rm.epi.min.r{run} rm.epi.1.r{run}+orig\n",
    "    # below file is big, should be removed\n",
    "    ! rm rm.epi.1.r{run}+orig*\n",
    "\n",
    "\n",
    "\n",
    "# 变化之后，处理一下mask的问题\n",
    "    # ----------------------------------------\n",
    "# create the extents mask: mask_epi_extents+orig\n",
    "# (this is a mask of voxels that have valid data at every TR,\n",
    "# there might be some pixel out of extents during mc)\n",
    "! 3dMean -datum short -prefix rm.epi.mean rm.epi.min.r*.HEAD\n",
    "# and apply the extents mask to the EPI data\n",
    "! 3dcalc -a rm.epi.mean+orig -expr \"step(a-0.999)\" -prefix mask_epi_extents\n",
    "\n",
    "# 去掉mask之外的voxel的数据\n",
    "for run in runstr:\n",
    "    ! 3dcalc -a rm.epi.nomask.r{run}+orig -b mask_epi_extents+orig \\\n",
    "           -expr \"a*b\" -prefix pb05.{subj}.r{run}.al2anat\n",
    "    # save to nifti format seems to reduce file size\n",
    "\n",
    "# rm.epi.nomask are big files, remove them\n",
    "! rm rm.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. step 7: epi volume 2 surface\n",
    "\n",
    "   上一步我们已经把epi数据划到了和个体的结构像一个空间。很多时候我们需要做基于surface的数据分析。我们进一步把三维的epi数据插值划到surface上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_dir = FREESURFER_HOME+f'/subjects/{subj}/SUMA'\n",
    "# map volume data to the surface of each hemisphere\n",
    "for hemi in ['lh', 'rh']:\n",
    "    for  run in runstr:\n",
    "        ! 3dVol2Surf -spec {surface_dir}/std.141.{subj}_{hemi}.spec   \\\n",
    "                   -sv {subj}_SurfVol+orig           \\\n",
    "                   -surf_A smoothwm                            \\\n",
    "                   -surf_B pial                                \\\n",
    "                   -f_index nodes                              \\\n",
    "                   -f_steps 10                                 \\\n",
    "                   -map_func ave                               \\\n",
    "                   -oob_value 0                                \\\n",
    "                   -grid_parent pb05.{subj}.r{run}.al2anat+orig   \\\n",
    "                   -out_niml pb05.{subj}.{hemi}.r{run}.surf.niml.dset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. step 8: epi 2 MNI anat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Transform epi to match anat, add by RZ ===============\n",
    "# In this step we need to concatenate Distortion(opt)+motion+affine transformations\n",
    "# align each dset to base volume, then we separate whether we want to further align epi to anat\n",
    "for run in runstr:\n",
    "\n",
    "    # we apply distortion correction + motion correction + anat2epi + nonlinear warp to MNI transformation\n",
    "    # 注意我们这里是相反的顺序来concatenate的\n",
    "    cmd = f'3dNwarpApply -quintic -nwarp \"anatQQ.{subj}_WARP.nii anatQQ.{subj}.aff12.1D mat.r{run}.warp.aff12.1D blip_warp.r{run}_WARP+orig\" \\\n",
    "            -master MNI152_2009_template_SSW.nii.gz \\\n",
    "            -dxyz {resolution} \\\n",
    "            -source pb02.{subj}.r{run}.tshift+orig \\\n",
    "            -prefix rm.epi.nomask.r{run}'\n",
    "    unix_wrapper(cmd)\n",
    "\n",
    "    # create an all-1 dataset to mask the extents of the warp\n",
    "    ! 3dcalc -overwrite -a pb02.{subj}.r{run}.tshift+orig -expr 1 -prefix rm.epi.all1\n",
    "\n",
    "    # we apply distortion correction + motion correction + anat2epi transformation\n",
    "    cmd = f'3dNwarpApply -quintic -nwarp \"anatQQ.{subj}_WARP.nii anatQQ.{subj}.aff12.1D mat.r{run}.warp.aff12.1D blip_warp.r{run}_WARP+orig\" \\\n",
    "            -master MNI152_2009_template_SSW.nii.gz \\\n",
    "            -dxyz {resolution} \\\n",
    "            -source rm.epi.all1+orig         \\\n",
    "            -ainterp NN -quiet \\\n",
    "            -prefix rm.epi.1.r{run}'\n",
    "    unix_wrapper(cmd)\n",
    "\n",
    "    # make an extents intersection mask of this run across time domain\n",
    "    # this makes a mask that all volumes in this run have valid numbers\n",
    "    ! 3dTstat -min -prefix rm.epi.min.r{run} rm.epi.1.r{run}+tlrc\n",
    "    # below file is big, should be removed\n",
    "    ! rm rm.epi.1.r{run}+tlrc*\n",
    "\n",
    "\n",
    "\n",
    "# 变化之后，处理mask的问题\n",
    "# ----------------------------------------\n",
    "# create the extents mask: mask_epi_extents+orig\n",
    "# (this is a mask of voxels that have valid data at every TR,\n",
    "# there might be some pixel out of extents during mc)\n",
    "! 3dMean -datum short -prefix rm.epi.mean rm.epi.min.r*.HEAD\n",
    "# and apply the extents mask to the EPI data\n",
    "! 3dcalc -a rm.epi.mean+tlrc -expr \"step(a-0.999)\" -prefix mask_epi_2mni_extents\n",
    "\n",
    "# 去掉mask之外的voxel的数据\n",
    "for run in runstr:\n",
    "    ! 3dcalc -a rm.epi.nomask.r{run}+tlrc -b mask_epi_2mni_extents+tlrc \\\n",
    "           -expr \"a*b\" -prefix pb06.{subj}.r{run}.al2mni\n",
    "    # save to nifti format seems to reduce file size\n",
    "\n",
    "# rm.epi.nomask are big files, remove them\n",
    "! rm rm.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 补充步骤，用于进行smooth操作，  2022.7.3添加\n",
    "# 目的是对上一步step8所生成的pb06进行smooth   生成pb06s\n",
    "# 再对上上一步step7所生成的pb05***surf.niml文件进行smooth  生成pb05s*rh  和 pb05s*lh\n",
    "# 再对上上一步step7所生成的pb05.subj.r0*.al2anat+orig文件进行smooth  生成pb05s*.subj.r0*.blur+al2anat+orig\n",
    "\n",
    "for run in runstr:\n",
    "    ! 3dmerge -1blur_fwhm 4.0 -doall -prefix pb06s.{subj}.r{run}.blur+al2mni+tlrc \\\n",
    "        pb06.{subj}.r{run}.al2mni+tlrc\n",
    "\n",
    "for run in runstr:\n",
    "    ! 3dmerge -1blur_fwhm 4.0 -doall -prefix pb05s.{subj}.lh.r{run}.blur.surf.niml.dset \\\n",
    "        pb05.{subj}.lh.r{run}.surf.niml.dset\n",
    "\n",
    "for run in runstr:\n",
    "    ! 3dmerge -1blur_fwhm 4.0 -doall -prefix pb05s.{subj}.rh.r{run}.blur.surf.niml.dset \\\n",
    "        pb05.{subj}.rh.r{run}.surf.niml.dset\n",
    "\n",
    "#以下代码可以不用跑\n",
    "for run in runstr:\n",
    "    ! 3dmerge -1blur_fwhm 4.0 -doall -prefix pb05s.{subj}.r{run}.blur+al2anat+orig \\\n",
    "        pb05.{subj}.r{run}.al2anat+orig\n",
    "\n",
    "\n",
    "'''  \n",
    "#afni官网上的对于surface数据进行smooth的代码\n",
    "\n",
    "foreach hemi ( lh rh )\n",
    "    foreach run ( $runs )\n",
    "        # to save time, estimate blur parameters only once\n",
    "        if ( ! -f surf.smooth.params.1D ) then\n",
    "            SurfSmooth -spec $surface_dir/std.60.FT_${hemi}.spec         \\\n",
    "                       -surf_A smoothwm                                  \\\n",
    "                       -input pb03.$subj.$hemi.r$run.surf.niml.dset      \\\n",
    "                       -met HEAT_07                                      \\\n",
    "                       -target_fwhm 6.0                                  \\\n",
    "                       -blurmaster pb03.$subj.$hemi.r$run.surf.niml.dset \\\n",
    "                       -detrend_master                                   \\\n",
    "                       -output pb04.$subj.$hemi.r$run.blur.niml.dset     \\\n",
    "                       | tee surf.smooth.params.1D\n",
    "        else\n",
    "            set params = `1dcat surf.smooth.params.1D`\n",
    "            SurfSmooth -spec $surface_dir/std.60.FT_${hemi}.spec         \\\n",
    "                       -surf_A smoothwm                                  \\\n",
    "                       -input pb03.$subj.$hemi.r$run.surf.niml.dset      \\\n",
    "                       -met HEAT_07                                      \\\n",
    "                       -Niter $params[1]                                 \\\n",
    "                       -sigma $params[2]                                 \\\n",
    "                       -output pb04.$subj.$hemi.r$run.blur.niml.dset\n",
    "        endif\n",
    "    end\n",
    "end\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Step 9: 创造一个所有epi和mni模板共有的mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Mask =================================\n",
    "# Create 'full_mask' dataset (union mask)\n",
    "for run in runstr:\n",
    "    ! 3dAutomask -prefix rm.mask_r{run}.nii.gz pb06.{subj}.r{run}.al2mni+tlrc\n",
    "\n",
    "# 创造一个所有功能像的run合起来的mask\n",
    "! 3dmask_tool -inputs rm.mask_r*.nii.gz -union -prefix full_mask.{subj}.nii.gz\n",
    "! rm rm.mask*\n",
    "\n",
    "# ---- create subject anatomy mask, mask_anat.$subj+orig ----\n",
    "#      (resampled from aligned anat)\n",
    "# 把功能像resample到上面mask的分辨率\n",
    "! 3dresample -master full_mask.{subj}.nii.gz -input \\\n",
    "           MNI152_2009_template_SSW.nii.gz -prefix rm.resam.anat.nii.gz\n",
    "# convert to binary anat mask; fill gaps and holes\n",
    "! 3dmask_tool -dilate_input 5 -5 -fill_holes -input rm.resam.anat.nii.gz \\\n",
    "            -prefix mask_anat.{subj}.nii.gz\n",
    "\n",
    "# 结合功能像和结构像的mask\n",
    "# compute tighter EPI mask by intersecting with anat mask\n",
    "! 3dmask_tool -input full_mask.{subj}.nii.gz mask_anat.{subj}.nii.gz \\\n",
    "            -inter -prefix mask_epi_anat.{subj}.nii.gz\n",
    "# note Dice coefficient of masks, as well\n",
    "! 3ddot -dodice full_mask.{subj}.nii.gz mask_anat.{subj}.nii.gz > out.mask_ae_dice.txt\n",
    "\n",
    "# 删掉多余文件\n",
    "! rm rm*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Step 10: scale\n",
    "    原始EPI图像的数据可能很大，从几百到4000不等。但是我们一般不考虑这个绝对强度，考虑的是信号增长的比例。所以把数据scale一下, 让所有的voxel的时间序列均值为100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale each voxel time series to have a mean of 100\n",
    "# (be sure no negatives creep in)\n",
    "# (subject to a range of [0,200])\n",
    "\n",
    "# scale volume data\n",
    "for run in runstr:\n",
    "    ! 3dTstat -prefix rm.mean_r{run} pb06.{subj}.r{run}.al2mni+tlrc\n",
    "    ! 3dcalc -a pb06.{subj}.r{run}.al2mni+tlrc -b rm.mean_r{run}+tlrc \\\n",
    "           -c mask_epi_2mni_extents+tlrc \\\n",
    "           -expr \"c * min(200, a/b*100)*step(a)*step(b)\" \\\n",
    "           -prefix pb07.{subj}.r{run}.scale\n",
    "\n",
    "# combine all datasets into one\n",
    "! 3dTcat -prefix all_runs.{subj}.nii.gz pb07.{subj}.r*.scale+tlrc*\n",
    "# remove redundant file\n",
    "! rm rm.mean*\n",
    "\n",
    "# 再scale surface data\n",
    "for hemi in ['lh', 'rh']:\n",
    "    for run in runstr:\n",
    "       ! 3dTstat -prefix rm.{hemi}.mean_r{run}.niml.dset    \\\n",
    "            pb05.{subj}.{hemi}.r{run}.surf.niml.dset\n",
    "       ! 3dcalc -a pb05.{subj}.{hemi}.r{run}.surf.niml.dset  \\\n",
    "               -b rm.{hemi}.mean_r{run}.niml.dset          \\\n",
    "               -expr 'min(200, a/b*100)*step(a)*step(b)' \\\n",
    "               -prefix pb07.{subj}.{hemi}.r{run}.scale.niml.dset\n",
    "\n",
    "# remove redundant file\n",
    "! rm rm.*mean* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到这一步，基本上最主要的预处理就完结了。下一步可以进一步针对surface数据跑GLM，因为afni的surface都是在freesurfer的模板上对其且标准化的，所以surface数据可以直接跑group analysis。也可以在已经划到MNI space上的volume数据跑GLM。以后在volume上跑GLM的结果可以直接用MNI152的T1像来可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funcpp03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 step1\n",
    "先跑一个GLM，把所有头动信息回归掉，然后计算一系列诊断信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'3dDeconvolve' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n",
      "'3dTproject' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n",
      "'rm' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "# ================================ regress =================================\n",
    "# compute de-meaned motion parameters (for use in regression)\n",
    "# in regress (detrend step)\n",
    "\n",
    "# ------------------------------\n",
    "# run the regression analysis. Note that stats results should be written as afni format\n",
    "\n",
    "! 3dDeconvolve -input pb07.{subj}.r*.scale+tlrc.HEAD \\\n",
    "    -mask mask_epi_anat.{subj}.nii.gz \\\n",
    "    -censor motion_{subj}_censor.1D \\\n",
    "    -ortvec motion_demean.1D mot_demean \\\n",
    "    -polort 2 \\\n",
    "    -num_stimts 0 \\\n",
    "    -rout \\\n",
    "    -fout -tout -x1D X.xmat.1D -xjpeg X.jpg \\\n",
    "    -x1D_uncensored X.nocensor.xmat.1D \\\n",
    "    -fitts motpoly.fitts.{subj} \\\n",
    "    -errts motpoly.errts.{subj} \\\n",
    "    -bucket motpoly.stats.{subj}\n",
    "\n",
    "# should add save a script here\n",
    "\n",
    "# -- use 3dTproject to project out regression matrix --\n",
    "! 3dTproject -polort 0 -input pb07.{subj}.r*.scale+tlrc.HEAD \\\n",
    "           -censor motion_{subj}_censor.1D -cenmode ZERO \\\n",
    "           -ort X.nocensor.xmat.1D -prefix motpoly.errts.{subj}.tproject\n",
    "# 3dTproject can also supply '-passband 0.01, 0.08' to band pass time seriest\n",
    "\n",
    "! rm -f motpoly.stats* motpoly.fitts* # remove some large files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. step2 计算temporal sigmal-to-noise ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# create a temporal signal to noise ratio dataset\n",
    "#    signal: if 'scale' block, mean should be 100\n",
    "#    noise : compute standard deviation of errts\n",
    "unix_wrapper(f'3dTstat -mean -prefix rm.signal.all.nii.gz all_runs.{subj}.nii.gz\"[{ktrs}]\"')\n",
    "unix_wrapper(f'3dTstat -stdev -prefix rm.noise.all.nii.gz motpoly.errts.{subj}.tproject+tlrc\"[{ktrs}]\"')\n",
    "! 3dcalc -a rm.signal.all.nii.gz \\\n",
    "       -b rm.noise.all.nii.gz \\\n",
    "       -c full_mask.{subj}.nii.gz \\\n",
    "       -expr \"c*a/b\" -prefix TSNR.{subj}.nii.gz\n",
    "\n",
    "# remove redundant files\n",
    "! rm rm.signal.all* rm.noise.all*\n",
    "! rm motpoly.errts* X*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. step3: clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== auto block: finalize ==========================\n",
    "# Remove temporary files\n",
    "! rm -f rm.* # finally remove some left-over files\n",
    "\n",
    "# We can further remove despike, tshift and volreg data at the end volreg are big\n",
    "! rm -f all_runs*\n",
    "! rm anatQQ* MNI152_2009_template_SSW.nii.gz\n",
    "! rm pb02* pb04* pb05* \n",
    "! rm epi_mean_tmp*\n",
    "\n",
    "# organize data\n",
    "\n",
    "\n",
    "# ==================== preprocessing done ==================================\n",
    "# return to previous directory\n",
    "os.chdir(cwd)\n",
    "os.environ['SHELL'] = orig_shell     # switch back to the original shell\n",
    "print(f'\\n=============== Preprocessing started: {start_time} ================\\n')\n",
    "print(f'\\n=============== Preprocessing finished: {gettimestr(\"full\")} ================\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5 (tags/v3.7.5:5c02a39a0b, Oct 15 2019, 00:11:34) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43945f80469130c119fb960c4295ae331e7fb51022383025e24ae21a08d5f82b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
